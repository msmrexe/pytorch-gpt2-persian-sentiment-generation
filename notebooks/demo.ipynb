{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPT-2 Sentiment-Controlled Persian Review: Guided Demo**\n",
    "\n",
    "This notebook provides a guided demonstration of the from-scratch GPT-2 project for generating sentiment-controlled food reviews. We will use the scripts in the `scripts/` directory to train and generate text.\n",
    "\n",
    "**Note:** Ensure you have installed all dependencies from `requirements.txt` in your virtual environment before running this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Download\n",
    "\n",
    "First, we need to authenticate with Kaggle and Hugging Face to download the dataset and tokenizer.\n",
    "\n",
    "1.  **Hugging Face:** Run `huggingface-cli login` in your terminal and enter your access token. This is required for the `meta-llama/Llama-3.3-70B-Instruct` tokenizer.\n",
    "2.  **Kaggle:** Make sure your `kaggle.json` API token is set up. You can download it from your Kaggle account page and place it in `~/.kaggle/kaggle.json`.\n",
    "\n",
    "Once authenticated, the `train.py` script will automatically handle downloading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the Model\n",
    "\n",
    "Now, let's run the training script. We will train for just **5 epochs** with a **batch size of 32** for this demo.\n",
    "\n",
    "This script will:\n",
    "1.  Download the Snappfood dataset (if not present).\n",
    "2.  Load the Llama 3.3 tokenizer and add sentiment tokens.\n",
    "3.  Build the from-scratch GPT-2 model.\n",
    "4.  Run training and evaluation for 5 epochs.\n",
    "5.  Save the best model to `models/best_gpt2_model.pt`.\n",
    "6.  Save loss plots to `models/training_loss_plots.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/train.py \\\n",
    "    --epochs 5 \\\n",
    "    --batch_size 32 \\\n",
    "    --lr 1e-4 \\\n",
    "    --n_embd 192 \\\n",
    "    --n_layer 3 \\\n",
    "    --n_head 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Training Artifacts\n",
    "\n",
    "After the script finishes, you should see the saved model and the loss plots. Let's display the loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "plot_path = \"../models/training_loss_plots.png\"\n",
    "\n",
    "if os.path.exists(plot_path):\n",
    "    print(f\"Displaying loss plot from {plot_path}\")\n",
    "    display(Image(filename=plot_path))\n",
    "else:\n",
    "    print(\"Loss plot not found. Did the training script run correctly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Text\n",
    "\n",
    "Now we can use the `generate.py` script to generate new comments using our trained model. We'll test both positive and negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generate Positive Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment positive \\\n",
    "    --num_samples 5 \\\n",
    "    --model_path \"../models/best_gpt2_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generate Negative Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment negative \\\n",
    "    --num_samples 5 \\\n",
    "    --model_path \"../models/best_gpt2_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Experiments\n",
    "\n",
    "The `generate.py` script allows you to control the generation process with `temperature`, `top_k`, and `top_p`. Let's try a few combinations to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Low Temperature (Conservative)\n",
    "\n",
    "Low temperature (e.g., 0.3) makes the model more deterministic and less creative. It will stick to high-probability, common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment positive \\\n",
    "    --num_samples 3 \\\n",
    "    --temperature 0.3 \\\n",
    "    --top_k 50 \\\n",
    "    --top_p 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. High Temperature (Creative/Risky)\n",
    "\n",
    "High temperature (e.g., 1.5) makes the model more creative and random. It's more likely to produce interesting, but also potentially nonsensical, text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment positive \\\n",
    "    --num_samples 3 \\\n",
    "    --temperature 1.5 \\\n",
    "    --top_k 50 \\\n",
    "    --top_p 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Low Top-K (Restrictive)\n",
    "\n",
    "Using a very small `top_k` (e.g., 5) restricts the model to only sampling from the 5 most likely next tokens. This can lead to very repetitive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment negative \\\n",
    "    --num_samples 3 \\\n",
    "    --temperature 0.8 \\\n",
    "    --top_k 5 \\\n",
    "    --top_p 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Low Top-P (Nucleus Sampling)\n",
    "\n",
    "Using a `top_p` of 0.5 means the model only samples from the smallest set of tokens whose cumulative probability exceeds 50%. This is an adaptive way to restrict the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate.py \\\n",
    "    --sentiment positive \\\n",
    "    --num_samples 3 \\\n",
    "    --temperature 0.8 \\\n",
    "    --top_k 0 \\\n",
    "    --top_p 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
